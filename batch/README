PREREQUISITES:
=============

See the main README file for how to build WikiTrust. 
Do a "make allopt" from the top level, as described there. 

To have more information on any command, use: 

<command> --help

ANALYZING LARGE WIKI DUMPSS WITH WIKITRUST:
==========================================

In general, to use WikiTrust on a wiki wiki dump you have two choices. 

One is to install the wiki as usual, install the WikiTrust extension,
use mwdumper to load the wiki dump in the database, and use
eval_online_wiki as explained in the top-level README file.  The
advantage of this approach is that it is simple.  On the other hand,
if you have many revisions (say, more than 100,000) in the wiki, the
approach causes quite a bit of database traffic, and overall, is not
very efficient.  You should expect a speed of 20 to 60 revisions per
second, depending on the disks available to the database.

The other approach consists in analyzing the dump file first, and then
loading it into the database once the analysis is complete.  This is
what is covered in this README file. 

Note that this is work in progress: we are modifying the code to
ensure that even a wiki as large as the English Wikipedia can be
analyzed in this fashion.  If you require help, contact the developers
at wikitrust-devel@googlegroups.com . 


SPLITTING THE WIKI DUMP:
=======================

For reasons of practicality, we split a Wikipedia dump into smaller
dumps before processing.  

mkdir ~/work/wiki-data/split_wikis
gunzip -c ~/work/wiki-data/wikis/my-big-wiki.xml.gz | \
   ./splitwiki -n 100 -p ~/work/wiki-data/split_wikis

This splits a wiki in chunks of 100 pages, compressing with gzip the
results. Obviously, use the appropriate decompress program above; the
intent is to decompress to standard output.

COMPUTING THE REDUCED STATISTCS:
===============================

The reduced statistics are a concise summary of the actions by authors
on wiki text.  They can be used to find out whose contributions are
preserved, and whose contributions are reverted.  To compute them, do
something like this for each of the 100-article chunks (or, if you
wish, for the whole wiki at once!): 

/bin/gunzip -c /home/luca/wiki-data/split_wiki/000/wiki-00000000.xml.gz \
    | batch/analysis/evalwiki -compute_stats -si ~/wiki-data/enwork/stats/wiki-00000000.stats

If you have multiple CPU cores, you can speed up the process by
adapting the script in util/do_all_en.py . This is not yet properly
documented, but it is fairly easy to figure out how to use the
script. 

SORTING THE REDUCED STATISTICS:
==============================

batch/analysis/combinestats \
    -bucket_dir ~/wiki-data/enwork/buckets/ \
    -input_dir ~/wiki-data/enwork/stats/ \
    -n_digits 4 -use_subdirs

This gives you roughly 20,000 files, and is suited to large wikis. 
For smaller wikis, use -n_digits 5 , which leads to 2,000 files.


COMPUTING THE REPUTATION FROM A STATISTICS FILE: 
===============================================

batch/analysis/generate_reputation -u ~/wiki-data/enwork/reps/rep_history.txt \
    -write_final_reps -gen_exact_rep -buckets ~/wiki-data/enwork/buckets/ 

Don't worry about the output; it is some testing information we use 
to judge the performance of the reputation.  Disregard it. 




### This README was revised up to here.  Please wait for the
### developers to update the part below before proceeding.









NOTES (optional reading!): 
    If users are anonymous, the reputation of their domains can be computed 
    using the following additional commands to generate_reputation:

    -domains

    The parameter -domains will ensure that anonymous user domains is included
    in the computation of reputations. In such cases, a unique user id is
    generated from their domain ip addresses.

    -ip_nbytes <n>

    The parameter -ip_nbytes can be used to specify the number of bytes [1, 4]
    that should be used to generate the unique user ids. For instance, the 
    address 172.162.56.61, with -ip_nbytes set to 1, will generate as user id
    -172, whereas the same address with -ip_nbytes set to 3, will generate
    a user id -11313720.

    The generate_reputation program has many command-line options, some of
    them referring to the algorithms to be used for reputation
    computation; do ./generate_reputation -help for more information.
    These options are not documented here. 


COLORING A WIKI ACCORDING TO TRUST: 
==================================

Coloring a wiki takes as input a compressed .xml dump (compressed with
gzip, by default), and produces an uncompressed .xml file containing
the colored markup language.  The resulting file can be loaded into a
mediawiki database using mwdumper (see below for help). 

There are many coefficients that determine how to color a wiki file. 
The following is an example that uses many of the available options: 

./evalwiki -d ~/work/wiki-data/colored_wikis/ -color_local_trust -historyfile ~/work/wiki-data/rep-histories/enwiki-20070206-rel1.0-users -rep_lends_trust 0.4 -trust_read_all 0.2 -trust_read_part 0.2 -trust_radius 2.0 -trust_part_radius 4.0 -n_rev_to_color 50 ~/work/wiki-data/split_wikis/enwiki-20070206_00066.xml.gz

Note that one can use a regexp, i.e.,
~/work/wiki-data/split_wikis/enwiki-20070206_* to color many wikis at
once.

Some parameters are worth noticing: 

This tells that we want to produce trust coloring.  If we also want to
have provenance information, use -trust_and_origin as the flag.

  -color_local_trust 

These parameters describe the numerical method used for trust
coloring.  They have reasonable defaults.  Look at the technical
report on trust computation for more information. 

  -rep_lends_trust 0.4 
  -trust_read_all 0.2 
  -trust_read_part 0.2 
  -trust_radius 2.0 
  -trust_part_radius 4.0

This says that, for each article, we have to output only the most
recent 50 revisions:

  -n_rev_to_color 50

This is the file with the history of user reputations.  It has to
match the wiki being colored!

  -historyfile ~/work/wiki-data/rep-histories/enwiki-20070206-rel1.0-users


LOADING THE COLORED WIKI IN A DATABASE: 
======================================

This varies somewhat depending on the name and password of the local
database.

First, reset the databaseif you have already loaded the same
articles, or if you wish to empty it:

scripts/reset_wiki.sh

Then, load the wiki: 

/load_wiki.sh ~/work/wiki-data/colored_wikis/enwiki-20070206_00066.xml

And hope that it all went well. 

EXTRACTING AUTHOR CONTRIBUTIONS
===============================

This feature can be used to extract author contributions as the weighted
average of the longevity of edits, weighted by the quantity of such edits.
This can be done from the statistics extracted through evalwiki. The 
following command can be used for this purpose -

./generate_reputation [-a] -u_contrib <filename> [-u_contrib_order_asc]
 <statistics_filename>

The -u_contrib <filename> notifies generate_reputation that we would like
to extract author contributions, by default in descending order into the
file named <filename>. 

The parameter -a is optional, which when present will include anonymous
users in the analysis.

The parameter -u_contrib_order_asc is optional, which when present
changes the order in which authors are listed in the contributions file
from being descending to ascending.

The final parameter <statistics_filename> is the name of the file with
statistics extracted from the wiki pages. These statistics can be 
extracted using evalwiki as follows -

./evalwiki -d <dest_dirname> -compute_stats <wiki_filename>

Here, the -d is used to specify <dest_dirname> as the directory in which 
we would like to evalwiki to place the extracted statistics file.

The -compute_stats switch instructs evalwiki to extract statistics.

The <wiki_filename> is the name of the file that contains the wiki data.
This can be either the wiki data xml file or a gzipped version of it.

A file called contributions.txt resides in the same directory as this
README file and contains an example of the file extracted with author
names and contributions. The file was generated from a small sample
statistics file and includes anonymous authors.
